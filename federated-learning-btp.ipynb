{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(dirname)\n        print(filename)\n#         print(filenames)\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-01T02:42:25.481556Z","iopub.execute_input":"2023-09-01T02:42:25.481964Z","iopub.status.idle":"2023-09-01T02:43:09.104168Z","shell.execute_reply.started":"2023-09-01T02:42:25.481932Z","shell.execute_reply":"2023-09-01T02:43:09.103262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls /kaggle/input/mnistasjpg\n","metadata":{"execution":{"iopub.status.busy":"2023-09-01T04:54:59.191886Z","iopub.execute_input":"2023-09-01T04:54:59.192283Z","iopub.status.idle":"2023-09-01T04:54:59.518025Z","shell.execute_reply.started":"2023-09-01T04:54:59.192257Z","shell.execute_reply":"2023-09-01T04:54:59.516132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install torch torchvision\n\nimport torch\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\nfrom torchvision.datasets import ImageFolder, DatasetFolder\nimport torchvision.transforms.functional as TF\nimport torchvision\n\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-01T04:54:59.550964Z","iopub.execute_input":"2023-09-01T04:54:59.551344Z","iopub.status.idle":"2023-09-01T04:55:14.294027Z","shell.execute_reply.started":"2023-09-01T04:54:59.551314Z","shell.execute_reply":"2023-09-01T04:55:14.292656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data=('/kaggle/input/mnistasjpg/trainingSample')\n","metadata":{"execution":{"iopub.status.busy":"2023-09-01T05:09:47.067972Z","iopub.execute_input":"2023-09-01T05:09:47.068397Z","iopub.status.idle":"2023-09-01T05:09:47.073825Z","shell.execute_reply.started":"2023-09-01T05:09:47.068364Z","shell.execute_reply":"2023-09-01T05:09:47.072604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls /kaggle/input/mnistasjpg/trainingSample/","metadata":{"execution":{"iopub.status.busy":"2023-09-01T05:09:47.558593Z","iopub.execute_input":"2023-09-01T05:09:47.559148Z","iopub.status.idle":"2023-09-01T05:09:47.845608Z","shell.execute_reply.started":"2023-09-01T05:09:47.559115Z","shell.execute_reply":"2023-09-01T05:09:47.844274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nnew_folder_path_2 = \"/kaggle/working/test01\"\n\n# Create the new folder if it doesn't exist\nif not os.path.exists(new_folder_path_2):\n    os.makedirs(new_folder_path_2)\n    \nnew_folder_path_3 = \"/kaggle/working/test02\"\n\n# Create the new folder if it doesn't exist\nif not os.path.exists(new_folder_path_3):\n    os.makedirs(new_folder_path_3)","metadata":{"execution":{"iopub.status.busy":"2023-09-01T05:09:48.143489Z","iopub.execute_input":"2023-09-01T05:09:48.144418Z","iopub.status.idle":"2023-09-01T05:09:48.153448Z","shell.execute_reply.started":"2023-09-01T05:09:48.144325Z","shell.execute_reply":"2023-09-01T05:09:48.151211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_folder_path_4= \"/kaggle/working/test01/x_test01\"\nnew_folder_path_5=\"/kaggle/working/test01/y_test01\"\n\n# Create the new folder if it doesn't exist\nif not os.path.exists(new_folder_path_4):\n    os.makedirs(new_folder_path_4)\n\nif not os.path.exists(new_folder_path_5):\n    os.makedirs(new_folder_path_5)\n    \nnew_folder_path_6= \"/kaggle/working/test02/x_test02\"\nnew_folder_path_7=\"/kaggle/working/test02/y_test02\"\n\nif not os.path.exists(new_folder_path_6):\n    os.makedirs(new_folder_path_6)\n    \nif not os.path.exists(new_folder_path_7):\n    os.makedirs(new_folder_path_7)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-01T05:09:48.993227Z","iopub.execute_input":"2023-09-01T05:09:48.993740Z","iopub.status.idle":"2023-09-01T05:09:49.001667Z","shell.execute_reply.started":"2023-09-01T05:09:48.993700Z","shell.execute_reply":"2023-09-01T05:09:49.000755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\noriginal_test_folder = \"/kaggle/input/mnistasjpg/testSample/testSample\"\nclient1TestData=\"/kaggle/working/test01/x_test01\"\n\nclient1_filenames = ['img_102.jpg','img_124.jpg','img_110.jpg','img_127.jpg','img_146.jpg',\n                     'img_150.jpg','img_160.jpg','img_173.jpg','img_182.jpg','img_186.jpg',\n                     'img_19.jpg','img_194.jpg','img_197.jpg','img_200.jpg','img_202.jpg',\n                     'img_204.jpg','img_205.jpg','img_207.jpg','img_21.jpg','img_212.jpg',\n                     'img_233.jpg','img_239.jpg','img_241.jpg','img_244.jpg','img_248.jpg',\n                     'img_251.jpg','img_255.jpg','img_268.jpg','img_275.jpg','img_279.jpg',\n                     'img_282.jpg','img_317.jpg','img_320.jpg','img_324.jpg','img_325.jpg',\n                     'img_336.jpg','img_344.jpg','img_44.jpg','img_45.jpg','img_47.jpg',\n                     'img_53.jpg','img_55.jpg','img_68.jpg','img_73.jpg','img_74.jpg',\n                     'img_76.jpg','img_77.jpg','img_78.jpg','img_85.jpg','img_98.jpg']\nfor filename in client1_filenames:\n    source_path = os.path.join(original_test_folder, filename)\n    destination_path = os.path.join(client1TestData, filename)\n    shutil.copy(source_path, destination_path)\n    \n","metadata":{"execution":{"iopub.status.busy":"2023-09-01T05:09:49.845949Z","iopub.execute_input":"2023-09-01T05:09:49.846388Z","iopub.status.idle":"2023-09-01T05:09:49.950398Z","shell.execute_reply.started":"2023-09-01T05:09:49.846356Z","shell.execute_reply":"2023-09-01T05:09:49.948274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_path = os.path.join('/kaggle/working/test01/y_test01', \"values.txt\")\nvalues = [\"9\", \"8\", \"8\",\"8\",\"8\",\n          \"8\",\"8\",\"8\",\"8\",\"9\",\n          \"9\",\"9\",\"9\",\"9\",\"9\",\n          \"9\",\"9\",\"8\",\"9\",\"9\",\n          \"9\",\"8\",\"8\",\"8\",\"8\",\n          \"9\",\"8\",\"8\",\"9\",\"8\",\n          \"9\",\"8\",\"8\",\"9\",\"8\",\n          \"9\",\"8\",\"9\",\"8\",\"8\",\n          \"8\",\"8\",\"9\",\"8\",\"8\",\n          \"8\",\"9\",\"9\",\"9\",\"9\"\n         ]\n\nwith open(file_path, \"w\") as file:\n    for value in values:\n        file.write(value + \"\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-09-01T05:09:51.096315Z","iopub.execute_input":"2023-09-01T05:09:51.096823Z","iopub.status.idle":"2023-09-01T05:09:51.108345Z","shell.execute_reply.started":"2023-09-01T05:09:51.096787Z","shell.execute_reply":"2023-09-01T05:09:51.105662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"original_test_folder = \"/kaggle/input/mnistasjpg/testSample/testSample\"\nclient2TestData=\"/kaggle/working/test02/x_test02\"\n\nclient2_filenames = ['img_1.jpg','img_10.jpg','img_277.jpg','img_274.jpg','img_270.jpg',\n                     'img_27.jpg','img_269.jpg','img_263.jpg','img_262.jpg','img_258.jpg',\n                     'img_254.jpg','img_253.jpg','img_249.jpg','img_103.jpg','img_106.jpg',\n                     'img_115.jpg','img_117.jpg','img_118.jpg','img_119.jpg','img_120.jpg',\n                     'img_121.jpg','img_123.jpg','img_126.jpg','img_131.jpg','img_137.jpg',\n                     'img_138.jpg','img_139.jpg','img_142.jpg','img_149.jpg','img_152.jpg',\n                     'img_156.jpg','img_16.jpg','img_161.jpg','img_165.jpg','img_166.jpg',\n                     'img_169.jpg','img_17.jpg','img_171.jpg','img_175.jpg','img_176.jpg',\n                     'img_179.jpg','img_18.jpg','img_180.jpg','img_184.jpg','img_187.jpg',\n                     'img_188.jpg','img_192.jpg','img_201.jpg','img_210.jpg','img_213.jpg','img_215.jpg',\n                     'img_216.jpg','img_217.jpg','img_219.jpg','img_22.jpg','img_221.jpg',\n                     'img_222.jpg','img_223.jpg','img_224.jpg','img_225.jpg','img_229.jpg',\n                     'img_23.jpg','img_234.jpg','img_236.jpg','img_237.jpg','img_238.jpg',\n                     'img_243.jpg','img_283.jpg','img_286.jpg','img_288.jpg','img_316.jpg',\n                     'img_319.jpg','img_322.jpg','img_323.jpg','img_328.jpg','img_330.jpg',\n                     'img_332.jpg','img_333.jpg','img_334.jpg','img_34.jpg','img_342.jpg',\n                     'img_343.jpg','img_350.jpg','img_36.jpg','img_39.jpg','img_48.jpg',\n                     'img_5.jpg','img_54.jpg','img_56.jpg','img_57.jpg',\n                     'img_58.jpg','img_59.jpg','img_62.jpg','img_67.jpg','img_70.jpg',\n                     'img_75.jpg','img_79.jpg','img_8.jpg','img_80.jpg','img_83.jpg',\n                     'img_86.jpg','img_88.jpg','img_91.jpg','img_95.jpg','img_97.jpg']\nfor filename in client2_filenames:\n    source_path = os.path.join(original_test_folder, filename)\n    destination_path = os.path.join(client2TestData, filename)\n    shutil.copy(source_path, destination_path)","metadata":{"execution":{"iopub.status.busy":"2023-09-01T05:09:51.784650Z","iopub.execute_input":"2023-09-01T05:09:51.785481Z","iopub.status.idle":"2023-09-01T05:09:51.964506Z","shell.execute_reply.started":"2023-09-01T05:09:51.785445Z","shell.execute_reply":"2023-09-01T05:09:51.963054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_path = os.path.join('/kaggle/working/test02/y_test02', \"values.txt\")\nvalues = [\"2\",\"3\",\"1\",\"3\",\"1\",\n          \"2\",\"3\",\"3\",\"3\",\"1\",\n          \"3\",\"1\",\"3\",\"2\",\"3\",\n          \"1\",\"1\",\"1\",\"3\",\"3\",\n          \"3\",\"2\",\"3\",\"3\",\"3\",\n          \"1\",\"3\",\"3\",\"2\",\"3\",\n          \"3\",\"3\",\"3\",\"1\",\"3\",\n          \"3\",\"3\",\"2\",\"1\",\"3\",\n          \"1\",\"1\",\"2\",\"2\",\"2\",\n          \"1\",\"1\",\"1\",\"2\",\"3\",\"3\",\n          \"2\",\"1\",\"3\",\"1\",\"2\",\n          \"3\",\"1\",\"2\",\"2\",\"1\",\n          \"1\",\"1\",\"3\",\"2\",\"3\",\n          \"2\",\"2\",\"3\",\"1\",\"3\",\n          \"1\",\"1\",\"1\",\"1\",\"3\",\n          \"3\",\"1\",\"3\",\"2\",\"3\",\n          \"1\",\"2\",\"2\",\"1\",\"2\",\n          \"3\",\"3\",\"2\",\"1\",\n          \"2\",\"2\",\"1\",\"1\",\"1\",\n          \"2\",\"2\",\"3\",\"3\",\"1\",\n          \"2\",\"3\",\"2\",\"1\",\"3\",\n         ]\n\nwith open(file_path, \"w\") as file:\n    for value in values:\n        file.write(value + \"\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-09-01T05:09:52.302082Z","iopub.execute_input":"2023-09-01T05:09:52.302783Z","iopub.status.idle":"2023-09-01T05:09:52.315993Z","shell.execute_reply.started":"2023-09-01T05:09:52.302732Z","shell.execute_reply":"2023-09-01T05:09:52.313802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((28, 28)), \n    transforms.Grayscale(num_output_channels=1), \n    transforms.ToTensor(),  \n    transforms.Normalize((0.5,), (0.5,)) \n])\n","metadata":{"execution":{"iopub.status.busy":"2023-09-01T05:09:52.887972Z","iopub.execute_input":"2023-09-01T05:09:52.888373Z","iopub.status.idle":"2023-09-01T05:09:52.895017Z","shell.execute_reply.started":"2023-09-01T05:09:52.888344Z","shell.execute_reply":"2023-09-01T05:09:52.893007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class CustomDataset_client01(ImageFolder):\n#     def __init__(self, root, transform=None):\n#         super().__init__(root, transform=transform)\n#         self.samples = [\n#             sample for sample in self.samples\n#             if os.path.basename(os.path.dirname(sample[0])) in ['8', '9']\n#         ]\n\nclass CustomDataset_client01(ImageFolder):\n    def __init__(self, root, transform=None):\n        super().__init__(root, transform=transform)\n        \n        filtered_samples = []\n        for sample in self.samples:\n            folder_name = os.path.basename(os.path.dirname(sample[0]))\n            if folder_name in ['8', '9']:\n                filtered_samples.append(sample)       \n        self.samples = filtered_samples\n        \nclass CustomDataset_client02(ImageFolder):\n    def __init__(self, root, transform=None):\n        super().__init__(root, transform=transform)\n        self.samples = [\n            sample for sample in self.samples\n            if os.path.basename(os.path.dirname(sample[0])) in ['1', '2','3']\n        ]","metadata":{"execution":{"iopub.status.busy":"2023-09-01T05:09:54.283327Z","iopub.execute_input":"2023-09-01T05:09:54.283664Z","iopub.status.idle":"2023-09-01T05:09:54.292545Z","shell.execute_reply.started":"2023-09-01T05:09:54.283639Z","shell.execute_reply":"2023-09-01T05:09:54.291422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_folder = \"/kaggle/input/mnistasjpg/trainingSample/trainingSample\"","metadata":{"execution":{"iopub.status.busy":"2023-09-01T05:09:54.827574Z","iopub.execute_input":"2023-09-01T05:09:54.827975Z","iopub.status.idle":"2023-09-01T05:09:54.832476Z","shell.execute_reply.started":"2023-09-01T05:09:54.827945Z","shell.execute_reply":"2023-09-01T05:09:54.831600Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 64","metadata":{"execution":{"iopub.status.busy":"2023-09-01T05:09:55.262621Z","iopub.execute_input":"2023-09-01T05:09:55.262957Z","iopub.status.idle":"2023-09-01T05:09:55.267516Z","shell.execute_reply.started":"2023-09-01T05:09:55.262934Z","shell.execute_reply":"2023-09-01T05:09:55.266406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomTestDataset(Dataset):\n    def __init__(self, image_folder, target_folder, filenames, transform=None):\n        self.image_folder = image_folder\n        self.target_folder = target_folder\n        self.filenames = filenames\n        self.transform = transform\n\n        self.targets = self.load_targets()\n\n    def load_targets(self):\n        targets = []\n        for filename in self.filenames:\n            target_path = os.path.join(self.target_folder, \"values.txt\")\n            with open(target_path, \"r\") as file:\n                target_value = int(file.readline().strip())\n                targets.append(target_value)\n        return targets\n\n    def __len__(self):\n        return len(self.filenames)\n\n    def __getitem__(self, idx):\n        image_filename = self.filenames[idx]\n        image_path = os.path.join(self.image_folder, image_filename)\n        image = Image.open(image_path).convert(\"L\")  # Open image in grayscale mode\n\n        if self.transform:\n            image = self.transform(image)\n\n        target = torch.tensor(self.targets[idx])  # Load target from pre-loaded list\n\n        return image, target","metadata":{"execution":{"iopub.status.busy":"2023-09-01T05:09:55.824185Z","iopub.execute_input":"2023-09-01T05:09:55.824703Z","iopub.status.idle":"2023-09-01T05:09:55.834978Z","shell.execute_reply.started":"2023-09-01T05:09:55.824647Z","shell.execute_reply":"2023-09-01T05:09:55.834035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"model","metadata":{}},{"cell_type":"code","source":"class SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n        self.conv_layers = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2),\n            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n        self.fc_layers = nn.Sequential(\n            nn.Linear(32 * 7 * 7, 128),\n            nn.ReLU(),\n            nn.Linear(128, 10)  # 10 classes for digits 0-9\n        )\n\n    def forward(self, x):\n        x = self.conv_layers(x)\n        x = x.view(x.size(0), -1)  # Flatten the tensor for fully connected layers\n        x = self.fc_layers(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-09-01T05:09:56.887107Z","iopub.execute_input":"2023-09-01T05:09:56.887889Z","iopub.status.idle":"2023-09-01T05:09:56.895324Z","shell.execute_reply.started":"2023-09-01T05:09:56.887856Z","shell.execute_reply":"2023-09-01T05:09:56.894186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Client-1","metadata":{}},{"cell_type":"code","source":"train_dataset_client01 = CustomDataset_client01(train_folder, transform=transform)\n# train_loader_1 = DataLoader(train_dataset_client01, batch_size=batch_size, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-01T05:09:58.262673Z","iopub.execute_input":"2023-09-01T05:09:58.263079Z","iopub.status.idle":"2023-09-01T05:09:58.288304Z","shell.execute_reply.started":"2023-09-01T05:09:58.263053Z","shell.execute_reply":"2023-09-01T05:09:58.287189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom torch.utils.data import SubsetRandomSampler\n\n# Assuming you have already defined train_loader_2, batch_size, and train_dataset_client02\n\n# Set the validation split ratio (e.g., 0.2 for 80% training and 20% validation)\nvalidation_split = 0.2\n\n# Compute the split indices\ndataset_size = len(train_dataset_client01)\nindices = list(range(dataset_size))\nsplit = int(np.floor(validation_split * dataset_size))\n\n# Shuffle the indices\nnp.random.shuffle(indices)\n\n# Create sampler objects for training and validation sets\ntrain_indices, val_indices = indices[split:], indices[:split]\ntrain_sampler = SubsetRandomSampler(train_indices)\nval_sampler = SubsetRandomSampler(val_indices)\n\n# Create data loaders for training and validation sets using the samplers\ntrain_loader_1 = DataLoader(train_dataset_client01, batch_size=batch_size, sampler=train_sampler)\nval_loader_1 = DataLoader(train_dataset_client01, batch_size=batch_size, sampler=val_sampler)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-01T05:09:58.876650Z","iopub.execute_input":"2023-09-01T05:09:58.877528Z","iopub.status.idle":"2023-09-01T05:09:58.888206Z","shell.execute_reply.started":"2023-09-01T05:09:58.877484Z","shell.execute_reply":"2023-09-01T05:09:58.885772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_folder = \"/kaggle/working/test01/x_test01\"\ntarget_folder = \"/kaggle/working/test01/y_test01\"\n\n\ntest_dataset_1 = CustomTestDataset(image_folder, target_folder, client1_filenames, transform=transform)","metadata":{"execution":{"iopub.status.busy":"2023-09-01T05:09:59.619705Z","iopub.execute_input":"2023-09-01T05:09:59.620441Z","iopub.status.idle":"2023-09-01T05:09:59.629896Z","shell.execute_reply.started":"2023-09-01T05:09:59.620401Z","shell.execute_reply":"2023-09-01T05:09:59.627636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_loader_1 = DataLoader(test_dataset_1, batch_size=batch_size, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-01T05:10:00.524861Z","iopub.execute_input":"2023-09-01T05:10:00.525287Z","iopub.status.idle":"2023-09-01T05:10:00.530822Z","shell.execute_reply.started":"2023-09-01T05:10:00.525256Z","shell.execute_reply":"2023-09-01T05:10:00.529843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model1 = SimpleCNN()\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model1.parameters(), lr=0.01)\n\nnum_epochs = 10\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel1.to(device)\n\ntrain_losses = []\nvalidation_accuracies = []\n\nfor epoch in range(num_epochs):\n    model1.train()\n    total_train_loss = 0.0\n    \n    for batch_idx, (data, target) in enumerate(train_loader_1):\n        data, target = data.to(device), target.to(device)\n\n        optimizer.zero_grad()\n        output = model1(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n\n        total_train_loss += loss.item()\n\n        if batch_idx % 100 == 0:\n            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader_1)}], Loss: {loss.item():.4f}')\n    \n    average_train_loss = total_train_loss / len(train_loader_1)\n    train_losses.append(average_train_loss)\n    \n    # Validation loop\n    model1.eval()\n    correct_val = 0\n    total_val = 0\n    \n    with torch.no_grad():\n        for data, target in val_loader_1:  # Use the validation loader\n            data, target = data.to(device), target.to(device)\n            output = model1(data)\n            _, predicted = torch.max(output.data, 1)\n            total_val += target.size(0)\n            correct_val += (predicted == target).sum().item()\n    \n    validation_accuracy = 100 * correct_val / total_val\n    \n    validation_accuracies.append(validation_accuracy)\n    \n    print(f'Epoch [{epoch+1}/{num_epochs}], '\n          f'Training Loss: {average_train_loss:.4f}, '\n          f'Validation Accuracy: {validation_accuracy:.2f}%')\n\n# Testing loop (after training)\nmodel1.eval()\ncorrect_test = 0\ntotal_test = 0\nall_predictions_test = []\n\nwith torch.no_grad():\n    for data, target in test_loader_1:\n        data, target = data.to(device), target.to(device)\n        output = model1(data)\n        _, predicted = torch.max(output.data, 1)\n        total_test += target.size(0)\n        correct_test += (predicted == target).sum().item()\n        all_predictions_test.extend(predicted.tolist())\n\ntest_accuracy = 100 * correct_test / total_test\n\nprint(f'Test Accuracy: {test_accuracy:.2f}%')\n","metadata":{"execution":{"iopub.status.busy":"2023-09-01T05:10:01.327665Z","iopub.execute_input":"2023-09-01T05:10:01.328340Z","iopub.status.idle":"2023-09-01T05:10:03.671673Z","shell.execute_reply.started":"2023-09-01T05:10:01.328302Z","shell.execute_reply":"2023-09-01T05:10:03.670662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12, 5))\n\n# Plotting loss curve\nplt.subplot(1, 2, 1)\nplt.plot(range(1, num_epochs+1), train_losses, label='Train Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training Loss')\nplt.legend()\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-01T05:10:03.673323Z","iopub.execute_input":"2023-09-01T05:10:03.673638Z","iopub.status.idle":"2023-09-01T05:10:03.953745Z","shell.execute_reply.started":"2023-09-01T05:10:03.673610Z","shell.execute_reply":"2023-09-01T05:10:03.951970Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"client-2","metadata":{}},{"cell_type":"code","source":"train_dataset_client02 = CustomDataset_client02(train_folder, transform=transform)\n# train_loader_2 = DataLoader(train_dataset_client02, batch_size=batch_size, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-01T05:10:03.955787Z","iopub.execute_input":"2023-09-01T05:10:03.956246Z","iopub.status.idle":"2023-09-01T05:10:03.978402Z","shell.execute_reply.started":"2023-09-01T05:10:03.956208Z","shell.execute_reply":"2023-09-01T05:10:03.976822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom torch.utils.data import SubsetRandomSampler\n\n# Assuming you have already defined train_loader_2, batch_size, and train_dataset_client02\n\n# Set the validation split ratio (e.g., 0.2 for 80% training and 20% validation)\nvalidation_split = 0.2\n\n# Compute the split indices\ndataset_size = len(train_dataset_client02)\nindices = list(range(dataset_size))\nsplit = int(np.floor(validation_split * dataset_size))\n\n# Shuffle the indices\nnp.random.shuffle(indices)\n\n# Create sampler objects for training and validation sets\ntrain_indices, val_indices = indices[split:], indices[:split]\ntrain_sampler = SubsetRandomSampler(train_indices)\nval_sampler = SubsetRandomSampler(val_indices)\n\n# Create data loaders for training and validation sets using the samplers\ntrain_loader_2 = DataLoader(train_dataset_client02, batch_size=batch_size, sampler=train_sampler)\nval_loader_2 = DataLoader(train_dataset_client02, batch_size=batch_size, sampler=val_sampler)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-01T05:10:04.170250Z","iopub.execute_input":"2023-09-01T05:10:04.170653Z","iopub.status.idle":"2023-09-01T05:10:04.180798Z","shell.execute_reply.started":"2023-09-01T05:10:04.170622Z","shell.execute_reply":"2023-09-01T05:10:04.179171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_dataset_client02)","metadata":{"execution":{"iopub.status.busy":"2023-09-01T05:10:04.800593Z","iopub.execute_input":"2023-09-01T05:10:04.801920Z","iopub.status.idle":"2023-09-01T05:10:04.809024Z","shell.execute_reply.started":"2023-09-01T05:10:04.801870Z","shell.execute_reply":"2023-09-01T05:10:04.807936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_folder = \"/kaggle/working/test02/x_test02\"\ntarget_folder = \"/kaggle/working/test02/y_test02\"\n\n\ntest_dataset_client2 = CustomTestDataset(image_folder, target_folder, client2_filenames, transform=transform)","metadata":{"execution":{"iopub.status.busy":"2023-09-01T05:10:05.751864Z","iopub.execute_input":"2023-09-01T05:10:05.752496Z","iopub.status.idle":"2023-09-01T05:10:05.765094Z","shell.execute_reply.started":"2023-09-01T05:10:05.752445Z","shell.execute_reply":"2023-09-01T05:10:05.761978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_loader_2 = DataLoader(test_dataset_client2, batch_size=batch_size, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-01T05:10:06.374934Z","iopub.execute_input":"2023-09-01T05:10:06.375391Z","iopub.status.idle":"2023-09-01T05:10:06.382049Z","shell.execute_reply.started":"2023-09-01T05:10:06.375357Z","shell.execute_reply":"2023-09-01T05:10:06.380592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(test_dataset_client2)","metadata":{"execution":{"iopub.status.busy":"2023-09-01T05:10:07.107876Z","iopub.execute_input":"2023-09-01T05:10:07.108337Z","iopub.status.idle":"2023-09-01T05:10:07.116787Z","shell.execute_reply.started":"2023-09-01T05:10:07.108296Z","shell.execute_reply":"2023-09-01T05:10:07.115131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model2 = SimpleCNN()\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model2.parameters(), lr=0.01)\n\nnum_epochs = 10\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel2.to(device)\n\ntrain_losses = []\nvalidation_accuracies = []\n\nfor epoch in range(num_epochs):\n    model2.train()\n    total_train_loss = 0.0\n    \n    for batch_idx, (data, target) in enumerate(train_loader_2):\n        data, target = data.to(device), target.to(device)\n\n        optimizer.zero_grad()\n        output = model2(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n\n        total_train_loss += loss.item()\n\n        if batch_idx % 100 == 0:\n            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader_2)}], Loss: {loss.item():.4f}')\n    \n    average_train_loss = total_train_loss / len(train_loader_2)\n    train_losses.append(average_train_loss)\n    \n    # Validation loop\n    model2.eval()\n    correct_val = 0\n    total_val = 0\n    \n    with torch.no_grad():\n        for data, target in val_loader_2:  # Use the validation loader\n            data, target = data.to(device), target.to(device)\n            output = model2(data)\n            _, predicted = torch.max(output.data, 1)\n            total_val += target.size(0)\n            correct_val += (predicted == target).sum().item()\n    \n    validation_accuracy = 100 * correct_val / total_val\n    \n    validation_accuracies.append(validation_accuracy)\n    \n    print(f'Epoch [{epoch+1}/{num_epochs}], '\n          f'Training Loss: {average_train_loss:.4f}, '\n          f'Validation Accuracy: {validation_accuracy:.2f}%')\n\n# Testing loop (after training)\nmodel2.eval()\ncorrect_test = 0\ntotal_test = 0\nall_predictions_test = []\n\nwith torch.no_grad():\n    for data, target in test_loader_2:\n        data, target = data.to(device), target.to(device)\n        output = model2(data)\n        _, predicted = torch.max(output.data, 1)\n        total_test += target.size(0)\n        correct_test += (predicted == target).sum().item()\n        all_predictions_test.extend(predicted.tolist())\n\ntest_accuracy = 100 * correct_test / total_test\n\nprint(f'Test Accuracy: {test_accuracy:.2f}%')\n","metadata":{"execution":{"iopub.status.busy":"2023-09-01T05:10:07.927008Z","iopub.execute_input":"2023-09-01T05:10:07.927637Z","iopub.status.idle":"2023-09-01T05:10:10.813789Z","shell.execute_reply.started":"2023-09-01T05:10:07.927592Z","shell.execute_reply":"2023-09-01T05:10:10.812460Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12, 5))\n\n# Plotting loss curve\nplt.subplot(1, 2, 1)\nplt.plot(range(1, num_epochs+1), train_losses, label='Train Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training Loss')\nplt.legend()\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-01T05:10:10.815434Z","iopub.execute_input":"2023-09-01T05:10:10.815778Z","iopub.status.idle":"2023-09-01T05:10:11.057520Z","shell.execute_reply.started":"2023-09-01T05:10:10.815748Z","shell.execute_reply":"2023-09-01T05:10:11.056877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Server","metadata":{}},{"cell_type":"markdown","source":"round-1","metadata":{}},{"cell_type":"code","source":"# Define the server class for combining models\nclass ModelServer(nn.Module):\n    def __init__(self, model1, model2, weights=(0.5, 0.5)):\n        super(ModelServer, self).__init__()\n        self.model1 = model1\n        self.model2 = model2\n        self.weights = weights\n\n    def forward(self, x):\n        features1 = self.model1.conv_layers(x)\n        features2 = self.model2.conv_layers(x)\n        \n        averaged_features = []\n        for f1, f2 in zip(features1, features2):\n            avg_feature = self.weights[0] * f1 + self.weights[1] * f2\n            averaged_features.append(avg_feature)\n        \n        averaged_features = torch.stack(averaged_features)\n        x = self.model1.fc_layers(averaged_features.view(x.size(0), -1))\n        return x\n\n\n# Instantiate the server with weighted averaging\nserver = ModelServer(model1, model2, weights=(0.5, 0.5))","metadata":{"execution":{"iopub.status.busy":"2023-09-01T05:10:25.728986Z","iopub.execute_input":"2023-09-01T05:10:25.729480Z","iopub.status.idle":"2023-09-01T05:10:25.738847Z","shell.execute_reply.started":"2023-09-01T05:10:25.729440Z","shell.execute_reply":"2023-09-01T05:10:25.737226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class ModelServer(nn.Module):\n#     def __init__(self, model1, model2, weights=(0.5, 0.5)):\n#         super(ModelServer, self).__init__()\n#         self.model1 = model1\n#         self.model2 = model2\n#         self.weights = weights\n\n#     def forward(self, x):\n#         fc_layers_model1 = list(server.model1.fc_layers.children())\n#         fc_layers_model2 = list(server.model2.fc_layers.children())\n\n# # Combine the fully connected layers' weights\n#         combined_fc_layers = []\n#         for fc1, fc2 in zip(fc_layers_model1, fc_layers_model2):\n#             combined_weights = (server.weights[0] * fc1.weight + server.weights[1] * fc2.weight)\n#             combined_bias = (server.weights[0] * fc1.bias + server.weights[1] * fc2.bias)\n    \n#             combined_fc = nn.Linear(fc1.in_features, fc1.out_features)\n#             combined_fc.weight = nn.Parameter(combined_weights)\n#             combined_fc.bias = nn.Parameter(combined_bias)\n    \n#             combined_fc_layers.append(combined_fc)\n\n# # Create a new model using the combined fully connected layers\n# improved_model = nn.Sequential(\n#     *list(server.model1.conv_layers.children()),  # Add conv layers from model1\n#     *combined_fc_layers  # Add combined fully connected layers\n# )","metadata":{"execution":{"iopub.status.busy":"2023-09-01T05:10:26.610607Z","iopub.execute_input":"2023-09-01T05:10:26.611784Z","iopub.status.idle":"2023-09-01T05:10:26.617581Z","shell.execute_reply.started":"2023-09-01T05:10:26.611725Z","shell.execute_reply":"2023-09-01T05:10:26.616127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_client_1 = server  # Assuming you've already defined 'server' as your model\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model_client_1.parameters(), lr=0.01)\n\nnum_epochs = 10\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel_client_1.to(device)\n\ntrain_losses = []\nvalidation_accuracies = []\n\nfor epoch in range(num_epochs):\n    model_client_1.train()\n    total_train_loss = 0.0\n    \n    for batch_idx, (data, target) in enumerate(train_loader_1):\n        data, target = data.to(device), target.to(device)\n\n        optimizer.zero_grad()\n        output = model_client_1(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n\n        total_train_loss += loss.item()\n\n        if batch_idx % 100 == 0:\n            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader_1)}], Loss: {loss.item():.4f}')\n    \n    average_train_loss = total_train_loss / len(train_loader_1)\n    train_losses.append(average_train_loss)\n    \n    # Validation loop\n    model_client_1.eval()\n    correct_val = 0\n    total_val = 0\n    \n    with torch.no_grad():\n        for data, target in val_loader_1:  # Use the validation loader for client 1\n            data, target = data.to(device), target.to(device)\n            output = model_client_1(data)\n            _, predicted = torch.max(output.data, 1)\n            total_val += target.size(0)\n            correct_val += (predicted == target).sum().item()\n    \n    validation_accuracy = 100 * correct_val / total_val\n    validation_accuracies.append(validation_accuracy)\n    \n    print(f'Epoch [{epoch+1}/{num_epochs}], '\n          f'Training Loss: {average_train_loss:.4f}, '\n          f'Validation Accuracy: {validation_accuracy:.2f}%')\n\n# Testing loop (after training)\nmodel_client_1.eval()\ncorrect_test = 0\ntotal_test = 0\nall_predictions_test = []\n\nwith torch.no_grad():\n    for data, target in test_loader_1:\n        data, target = data.to(device), target.to(device)\n        output = model_client_1(data)\n        _, predicted = torch.max(output.data, 1)\n        total_test += target.size(0)\n        correct_test += (predicted == target).sum().item()\n        all_predictions_test.extend(predicted.tolist())\n\ntest_accuracy = 100 * correct_test / total_test\n\nprint(f'Test Accuracy: {test_accuracy:.2f}%')\n","metadata":{"execution":{"iopub.status.busy":"2023-09-01T05:10:27.388858Z","iopub.execute_input":"2023-09-01T05:10:27.389592Z","iopub.status.idle":"2023-09-01T05:10:29.673086Z","shell.execute_reply.started":"2023-09-01T05:10:27.389564Z","shell.execute_reply":"2023-09-01T05:10:29.671994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12, 5))\n\n# Plotting loss curve\nplt.subplot(1, 2, 1)\nplt.plot(range(1, num_epochs+1), train_losses, label='Train Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training Loss')\nplt.legend()\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-01T05:10:31.246728Z","iopub.execute_input":"2023-09-01T05:10:31.247578Z","iopub.status.idle":"2023-09-01T05:10:31.528979Z","shell.execute_reply.started":"2023-09-01T05:10:31.247540Z","shell.execute_reply":"2023-09-01T05:10:31.526342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_client_2 = server  # Assuming you've already defined 'server' as your model\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model_client_2.parameters(), lr=0.1)\n\nnum_epochs = 10\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel_client_2.to(device)\n\nfor epoch in range(num_epochs):\n    model_client_2.train()\n    total_train_loss = 0.0\n    \n    for batch_idx, (data, target) in enumerate(train_loader_2):\n        data, target = data.to(device), target.to(device)\n\n        optimizer.zero_grad()\n        output = model_client_2(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n\n        total_train_loss += loss.item()\n\n        if batch_idx % 100 == 0:\n            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader_2)}], Loss: {loss.item():.4f}')\n    \n    average_train_loss = total_train_loss / len(train_loader_2)\n    \n    # Validation loop\n    model_client_2.eval()\n    correct_val = 0\n    total_val = 0\n    \n    with torch.no_grad():\n        for data, target in val_loader_2:  # Use the validation loader for client 1\n            data, target = data.to(device), target.to(device)\n            output = model_client_2(data)\n            _, predicted = torch.max(output.data, 1)\n            total_val += target.size(0)\n            correct_val += (predicted == target).sum().item()\n    \n    validation_accuracy = 100 * correct_val / total_val\n    \n    print(f'Epoch [{epoch+1}/{num_epochs}], '\n          f'Training Loss: {average_train_loss:.4f}, '\n          f'Validation Accuracy: {validation_accuracy:.2f}%')\n\n# Testing loop (after training)\nmodel_client_2.eval()\ncorrect_test = 0\ntotal_test = 0\nall_predictions_test = []\n\nwith torch.no_grad():\n    for data, target in test_loader_2:\n        data, target = data.to(device), target.to(device)\n        output = model_client_2(data)\n        _, predicted = torch.max(output.data, 1)\n        total_test += target.size(0)\n        correct_test += (predicted == target).sum().item()\n        all_predictions_test.extend(predicted.tolist())\n\ntest_accuracy = 100 * correct_test / total_test\n\nprint(f'Test Accuracy: {test_accuracy:.2f}%')\n","metadata":{"execution":{"iopub.status.busy":"2023-09-01T05:10:33.971046Z","iopub.execute_input":"2023-09-01T05:10:33.971538Z","iopub.status.idle":"2023-09-01T05:10:37.503099Z","shell.execute_reply.started":"2023-09-01T05:10:33.971500Z","shell.execute_reply":"2023-09-01T05:10:37.501768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12, 5))\n\n# Plotting loss curve\nplt.subplot(1, 2, 1)\nplt.plot(range(1, num_epochs+1), train_losses, label='Train Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training Loss')\nplt.legend()\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-01T05:10:39.505431Z","iopub.execute_input":"2023-09-01T05:10:39.506655Z","iopub.status.idle":"2023-09-01T05:10:39.773298Z","shell.execute_reply.started":"2023-09-01T05:10:39.506606Z","shell.execute_reply":"2023-09-01T05:10:39.771845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"round-2","metadata":{}},{"cell_type":"code","source":"server = ModelServer(model1, model2, weights=(0.25, 0.75))","metadata":{"execution":{"iopub.status.busy":"2023-09-01T05:12:05.235378Z","iopub.execute_input":"2023-09-01T05:12:05.235984Z","iopub.status.idle":"2023-09-01T05:12:05.243311Z","shell.execute_reply.started":"2023-09-01T05:12:05.235938Z","shell.execute_reply":"2023-09-01T05:12:05.241783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_client_1_2=server\n  # Assuming you've already defined 'server' as your model\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model_client_1_2.parameters(), lr=0.01)\n\nnum_epochs = 10\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel_client_1_2.to(device)\n\ntrain_losses = []\nvalidation_accuracies = []\n\nfor epoch in range(num_epochs):\n    model_client_1_2.train()\n    total_train_loss = 0.0\n    \n    for batch_idx, (data, target) in enumerate(train_loader_1):\n        data, target = data.to(device), target.to(device)\n\n        optimizer.zero_grad()\n        output = model_client_1_2(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n\n        total_train_loss += loss.item()\n\n        if batch_idx % 100 == 0:\n            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader_1)}], Loss: {loss.item():.4f}')\n    \n    average_train_loss = total_train_loss / len(train_loader_1)\n    train_losses.append(average_train_loss)\n    \n    # Validation loop\n    model_client_1_2.eval()\n    correct_val = 0\n    total_val = 0\n    \n    with torch.no_grad():\n        for data, target in val_loader_1:  # Use the validation loader for client 1\n            data, target = data.to(device), target.to(device)\n            output = model_client_1_2(data)\n            _, predicted = torch.max(output.data, 1)\n            total_val += target.size(0)\n            correct_val += (predicted == target).sum().item()\n    \n    validation_accuracy = 100 * correct_val / total_val\n    validation_accuracies.append(validation_accuracy)\n    \n    print(f'Epoch [{epoch+1}/{num_epochs}], '\n          f'Training Loss: {average_train_loss:.4f}, '\n          f'Validation Accuracy: {validation_accuracy:.2f}%')\n\n# Testing loop (after training)\nmodel_client_1_2.eval()\ncorrect_test = 0\ntotal_test = 0\nall_predictions_test = []\n\nwith torch.no_grad():\n    for data, target in test_loader_1:\n        data, target = data.to(device), target.to(device)\n        output = model_client_1_2(data)\n        _, predicted = torch.max(output.data, 1)\n        total_test += target.size(0)\n        correct_test += (predicted == target).sum().item()\n        all_predictions_test.extend(predicted.tolist())\n\ntest_accuracy = 100 * correct_test / total_test\n\nprint(f'Test Accuracy: {test_accuracy:.2f}%')\n","metadata":{"execution":{"iopub.status.busy":"2023-09-01T05:12:51.686621Z","iopub.execute_input":"2023-09-01T05:12:51.687128Z","iopub.status.idle":"2023-09-01T05:12:54.102887Z","shell.execute_reply.started":"2023-09-01T05:12:51.687093Z","shell.execute_reply":"2023-09-01T05:12:54.101845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12, 5))\n\n# Plotting loss curve\nplt.subplot(1, 2, 1)\nplt.plot(range(1, num_epochs+1), train_losses, label='Train Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training Loss')\nplt.legend()\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-01T05:13:14.218436Z","iopub.execute_input":"2023-09-01T05:13:14.218940Z","iopub.status.idle":"2023-09-01T05:13:14.500647Z","shell.execute_reply.started":"2023-09-01T05:13:14.218906Z","shell.execute_reply":"2023-09-01T05:13:14.499408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_client_2_2 = server  # Assuming you've already defined 'server' as your model\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model_client_2_2.parameters(), lr=0.1)\n\nnum_epochs = 10\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel_client_2_2.to(device)\n\nfor epoch in range(num_epochs):\n    model_client_2_2.train()\n    total_train_loss = 0.0\n    \n    for batch_idx, (data, target) in enumerate(train_loader_2):\n        data, target = data.to(device), target.to(device)\n\n        optimizer.zero_grad()\n        output = model_client_2_2(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n\n        total_train_loss += loss.item()\n\n        if batch_idx % 100 == 0:\n            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader_2)}], Loss: {loss.item():.4f}')\n    \n    average_train_loss = total_train_loss / len(train_loader_2)\n    \n    # Validation loop\n    model_client_2_2.eval()\n    correct_val = 0\n    total_val = 0\n    \n    with torch.no_grad():\n        for data, target in val_loader_2:  # Use the validation loader for client 1\n            data, target = data.to(device), target.to(device)\n            output = model_client_2_2(data)\n            _, predicted = torch.max(output.data, 1)\n            total_val += target.size(0)\n            correct_val += (predicted == target).sum().item()\n    \n    validation_accuracy = 100 * correct_val / total_val\n    \n    print(f'Epoch [{epoch+1}/{num_epochs}], '\n          f'Training Loss: {average_train_loss:.4f}, '\n          f'Validation Accuracy: {validation_accuracy:.2f}%')\n\n# Testing loop (after training)\nmodel_client_2_2.eval()\ncorrect_test = 0\ntotal_test = 0\nall_predictions_test = []\n\nwith torch.no_grad():\n    for data, target in test_loader_2:\n        data, target = data.to(device), target.to(device)\n        output = model_client_2_2(data)\n        _, predicted = torch.max(output.data, 1)\n        total_test += target.size(0)\n        correct_test += (predicted == target).sum().item()\n        all_predictions_test.extend(predicted.tolist())\n\ntest_accuracy = 100 * correct_test / total_test\n\nprint(f'Test Accuracy: {test_accuracy:.2f}%')\n","metadata":{"execution":{"iopub.status.busy":"2023-09-01T05:14:32.292349Z","iopub.execute_input":"2023-09-01T05:14:32.292718Z","iopub.status.idle":"2023-09-01T05:14:35.831479Z","shell.execute_reply.started":"2023-09-01T05:14:32.292672Z","shell.execute_reply":"2023-09-01T05:14:35.830008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12, 5))\n\n# Plotting loss curve\nplt.subplot(1, 2, 1)\nplt.plot(range(1, num_epochs+1), train_losses, label='Train Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training Loss')\nplt.legend()\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-01T05:15:12.665711Z","iopub.execute_input":"2023-09-01T05:15:12.666209Z","iopub.status.idle":"2023-09-01T05:15:12.944600Z","shell.execute_reply.started":"2023-09-01T05:15:12.666174Z","shell.execute_reply":"2023-09-01T05:15:12.942934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"round3","metadata":{}},{"cell_type":"code","source":"server = ModelServer(model1, model2, weights=(0.33, 0.67))","metadata":{"execution":{"iopub.status.busy":"2023-09-01T05:17:50.253282Z","iopub.execute_input":"2023-09-01T05:17:50.253649Z","iopub.status.idle":"2023-09-01T05:17:50.259277Z","shell.execute_reply.started":"2023-09-01T05:17:50.253620Z","shell.execute_reply":"2023-09-01T05:17:50.257801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_client_1_3=server\n  # Assuming you've already defined 'server' as your model\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model_client_1_3.parameters(), lr=0.001)\n\nnum_epochs = 10\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel_client_1_3.to(device)\n\ntrain_losses = []\nvalidation_accuracies = []\n\nfor epoch in range(num_epochs):\n    model_client_1_3.train()\n    total_train_loss = 0.0\n    \n    for batch_idx, (data, target) in enumerate(train_loader_1):\n        data, target = data.to(device), target.to(device)\n\n        optimizer.zero_grad()\n        output = model_client_1_3(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n\n        total_train_loss += loss.item()\n\n        if batch_idx % 100 == 0:\n            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader_1)}], Loss: {loss.item():.4f}')\n    \n    average_train_loss = total_train_loss / len(train_loader_1)\n    train_losses.append(average_train_loss)\n    \n    # Validation loop\n    model_client_1_3.eval()\n    correct_val = 0\n    total_val = 0\n    \n    with torch.no_grad():\n        for data, target in val_loader_1:  # Use the validation loader for client 1\n            data, target = data.to(device), target.to(device)\n            output = model_client_1_3(data)\n            _, predicted = torch.max(output.data, 1)\n            total_val += target.size(0)\n            correct_val += (predicted == target).sum().item()\n    \n    validation_accuracy = 100 * correct_val / total_val\n    validation_accuracies.append(validation_accuracy)\n    \n    print(f'Epoch [{epoch+1}/{num_epochs}], '\n          f'Training Loss: {average_train_loss:.4f}, '\n          f'Validation Accuracy: {validation_accuracy:.2f}%')\n\n# Testing loop (after training)\nmodel_client_1_3.eval()\ncorrect_test = 0\ntotal_test = 0\nall_predictions_test = []\n\nwith torch.no_grad():\n    for data, target in test_loader_1:\n        data, target = data.to(device), target.to(device)\n        output = model_client_1_3(data)\n        _, predicted = torch.max(output.data, 1)\n        total_test += target.size(0)\n        correct_test += (predicted == target).sum().item()\n        all_predictions_test.extend(predicted.tolist())\n\ntest_accuracy = 100 * correct_test / total_test\n\nprint(f'Test Accuracy: {test_accuracy:.2f}%')\n","metadata":{"execution":{"iopub.status.busy":"2023-09-01T05:17:51.474518Z","iopub.execute_input":"2023-09-01T05:17:51.474901Z","iopub.status.idle":"2023-09-01T05:17:53.683614Z","shell.execute_reply.started":"2023-09-01T05:17:51.474871Z","shell.execute_reply":"2023-09-01T05:17:53.682612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}